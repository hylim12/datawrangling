# -*- coding: utf-8 -*-
"""
# [Group_3]

Group Members: 
1. 1211111904 Eldeena Lim Huey Yinn
2. 1211109457 Do Wai Lung
3. 1221305730 Doris Heng
4. 1211108301 Kong Yi Xuan

"""
import pandas as pd
import numpy as np

file_path = 'wdbc.data'

df = pd.read_csv(file_path, delimiter=',')
df.columns =['ID',
             'Diagnosis',
             'radius1', 
             'texture1',
             'perimeter1',
             'area1', 
             'smoothness1',
             'compactness1',
             'concavity1',
             'concave_points1',
             'symmetry1',
             'fractal_dimension1',
             'radius2',
             'texture2',
             'perimeter2',
             'area2',
             'smoothness2',
             'compactness2',
             'concavity2',
             'concave_points2',
             'symmetry2',
             'fractal_dimension2',
             'radius3',
             'texture3',
             'perimeter3',
             'area3',
             'smoothness3',
             'compactness3',
             'concavity3', 
             'concave_points3',
             'symmetry3',
             'fractal_dimension3']
df

columns = [
    ('radius1'),
    ('texture1'),
    ('perimeter1'),
    ('area1'),
    ('smoothness1'),
    ('compactness1'),
    ('concavity1'),
    ('concave_points1'),
    ('symmetry1'),
    ('fractal_dimension1'),
    ('radius2'),
    ('texture2'),
    ('perimeter2'),
    ('area2'),
    ('smoothness2'),
    ('compactness2'),
    ('concavity2'),
    ('concave_points2'),
    ('symmetry2'),
    ('fractal_dimension2'),
    ('radius3'),
    ('texture3'),
    ('perimeter3'),
    ('area3'),
    ('smoothness3'),
    ('compactness3'),
    ('concavity3'),
    ('concave_points3'),
    ('symmetry3'),
    ('fractal_dimension3')
]

for column in columns:
     df[column] = df[column].fillna(df[column].median()) 

print(df.isna().sum())

from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SequentialFeatureSelector

# convert 'Diagnosis' column to binary: M = 1 (malignant), B = 0 (benign)
df['Diagnosis'] = df['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)

X = df.drop(columns=['Diagnosis', 'ID'])
y = df['Diagnosis']

linearreg = LinearRegression()

# forward selection with 4 features
sfs = SequentialFeatureSelector(linearreg, n_features_to_select=4, direction='forward')
sfs.fit(X, y)

selected_features = X.columns[sfs.get_support()]
print("Selected features (Forward Selection):", selected_features)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
np.random.seed(123)

data = pd.read_csv('wdbc.data')
print(data.head())
data = data.iloc[:,2:]
print(data.head())

cor = data.corr()
cor
plt.figure(figsize=(10,6))
sns.heatmap(cor, annot=True)
y = data.columns[1] 
s = abs(cor[y])
threshold = 0.5
selectedfeatures = s[s>0.5]
print("Selected features:", selectedfeatures)


from sklearn.linear_model import LassoCV
reg = LassoCV()
reg.fit(X,y)
print("Best alpha using built-in LassoCV: %f" % reg.alpha_)
print("Best score using built-in LassoCV: %f" %reg.score(X,y))
coef=pd.Series(reg.coef_, index=X.columns)
print(coef)
print("Lasso picked " + str(sum(coef != 0))+"variables and eliminated the other " + str(sum(coef == 0)) + " variables")

#the plot figure below shows which features are selected
imp_coef = coef.sort_values()
import matplotlib
matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.title("Feature importance using Lasso Model")


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lasso = Lasso(alpha=0.0001)  
lasso.fit(X_train_scaled, y_train)

print("Number of selected features:", np.sum(lasso.coef_ != 0))

plt.figure(figsize=(10, 6))
plt.bar(range(X_train.shape[1]), np.abs(lasso.coef_))
plt.title('Feature importance using Lasso Model')
plt.show()


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lasso = Lasso(alpha=0.0001)  
lasso.fit(X_train_scaled, y_train)

print("Number of selected features:", np.sum(lasso.coef_ != 0))

plt.figure(figsize=(10, 6))
plt.bar(range(X_train.shape[1]), np.abs(lasso.coef_))
plt.title('Feature importance using Lasso Model')
plt.show()
